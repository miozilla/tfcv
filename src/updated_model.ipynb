{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "def83718-2af7-44a0-a3bd-3e30527d66bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T07:33:22.270636Z",
     "iopub.status.busy": "2026-02-13T07:33:22.270122Z",
     "iopub.status.idle": "2026-02-13T07:34:35.269163Z",
     "shell.execute_reply": "2026-02-13T07:34:35.268189Z",
     "shell.execute_reply.started": "2026-02-13T07:33:22.270587Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.cloud.appengine_logging_v1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.appengine_logging_v1 past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "2026-02-13 07:33:23.258501: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-02-13 07:33:23.318368: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-02-13 07:33:23.318417: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-02-13 07:33:23.320050: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-02-13 07:33:23.329402: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-02-13 07:33:23.330873: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.5157 - sparse_categorical_accuracy: 0.8175\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3923 - sparse_categorical_accuracy: 0.8600\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3535 - sparse_categorical_accuracy: 0.8727\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3290 - sparse_categorical_accuracy: 0.8803\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3108 - sparse_categorical_accuracy: 0.8866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:upLogger:Model: \"sequential\"\n",
      "INFO:upLogger:_________________________________________________________________\n",
      "INFO:upLogger: Layer (type)                Output Shape              Param #   \n",
      "INFO:upLogger:=================================================================\n",
      "INFO:upLogger: flatten (Flatten)           (None, 784)               0         \n",
      "INFO:upLogger:                                                                 \n",
      "INFO:upLogger: dense (Dense)               (None, 64)                50240     \n",
      "INFO:upLogger:                                                                 \n",
      "INFO:upLogger: dense_1 (Dense)             (None, 10)                650       \n",
      "INFO:upLogger:                                                                 \n",
      "INFO:upLogger:=================================================================\n",
      "INFO:upLogger:Total params: 50890 (198.79 KB)\n",
      "INFO:upLogger:Trainable params: 50890 (198.79 KB)\n",
      "INFO:upLogger:Non-trainable params: 0 (0.00 Byte)\n",
      "INFO:upLogger:_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Import and configure logging\n",
    "import logging\n",
    "import google.cloud.logging as cloud_logging\n",
    "from google.cloud.logging.handlers import CloudLoggingHandler\n",
    "from google.cloud.logging_v2.handlers import setup_logging\n",
    "up_logger = logging.getLogger('upLogger')\n",
    "up_logger.setLevel(logging.INFO)\n",
    "up_logger.addHandler(CloudLoggingHandler(cloud_logging.Client(), name=\"updated\"))\n",
    "\n",
    "# Import tensorflow_datasets\n",
    "import tensorflow_datasets as tfds\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define, load and configure data\n",
    "(ds_train, ds_test), info = tfds.load('fashion_mnist', split=['train', 'test'], with_info=True, as_supervised=True)\n",
    "# Define batch size\n",
    "BATCH_SIZE = 32\n",
    "# Normalizing and batch processing of data\n",
    "ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)\n",
    "ds_test = ds_test.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)\n",
    "# Define the model\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "# Compile data\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(ds_train, epochs=5)\n",
    "# Logs model summary\n",
    "model.summary(print_fn=up_logger.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87803806-4402-4a53-87a8-59ccb35952c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T07:37:32.045064Z",
     "iopub.status.busy": "2026-02-13T07:37:32.042565Z",
     "iopub.status.idle": "2026-02-13T07:38:49.059519Z",
     "shell.execute_reply": "2026-02-13T07:38:49.058199Z",
     "shell.execute_reply.started": "2026-02-13T07:37:32.045014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.4965 - sparse_categorical_accuracy: 0.8235\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3747 - sparse_categorical_accuracy: 0.8649\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.3338 - sparse_categorical_accuracy: 0.8791\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3082 - sparse_categorical_accuracy: 0.8878\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.2893 - sparse_categorical_accuracy: 0.8934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:upLogger:Model: \"sequential_1\"\n",
      "INFO:upLogger:_________________________________________________________________\n",
      "INFO:upLogger: Layer (type)                Output Shape              Param #   \n",
      "INFO:upLogger:=================================================================\n",
      "INFO:upLogger: flatten_1 (Flatten)         (None, 784)               0         \n",
      "INFO:upLogger:                                                                 \n",
      "INFO:upLogger: dense_2 (Dense)             (None, 128)               100480    \n",
      "INFO:upLogger:                                                                 \n",
      "INFO:upLogger: dense_3 (Dense)             (None, 10)                1290      \n",
      "INFO:upLogger:                                                                 \n",
      "INFO:upLogger:=================================================================\n",
      "INFO:upLogger:Total params: 101770 (397.54 KB)\n",
      "INFO:upLogger:Trainable params: 101770 (397.54 KB)\n",
      "INFO:upLogger:Non-trainable params: 0 (0.00 Byte)\n",
      "INFO:upLogger:_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1st Experiment with different values for the dense layer.\n",
    "# Import and configure logging\n",
    "import logging\n",
    "import google.cloud.logging as cloud_logging\n",
    "from google.cloud.logging.handlers import CloudLoggingHandler\n",
    "from google.cloud.logging_v2.handlers import setup_logging\n",
    "up_logger = logging.getLogger('upLogger')\n",
    "up_logger.setLevel(logging.INFO)\n",
    "up_logger.addHandler(CloudLoggingHandler(cloud_logging.Client(), name=\"updated\"))\n",
    "\n",
    "# Import tensorflow_datasets\n",
    "import tensorflow_datasets as tfds\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define, load and configure data\n",
    "(ds_train, ds_test), info = tfds.load('fashion_mnist', split=['train', 'test'], with_info=True, as_supervised=True)\n",
    "# Define batch size\n",
    "BATCH_SIZE = 32\n",
    "# Normalizing and batch processing of data\n",
    "ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)\n",
    "ds_test = ds_test.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)\n",
    "# Define the model\n",
    "# change 64 to 128 neurons\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "# Compile data\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(ds_train, epochs=5)\n",
    "# Logs model summary\n",
    "model.summary(print_fn=up_logger.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfaf142a-5c63-49fa-969a-df0229d2c404",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T07:42:41.354010Z",
     "iopub.status.busy": "2026-02-13T07:42:41.352291Z",
     "iopub.status.idle": "2026-02-13T07:43:38.067275Z",
     "shell.execute_reply": "2026-02-13T07:43:38.066306Z",
     "shell.execute_reply.started": "2026-02-13T07:42:41.353923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.5048 - sparse_categorical_accuracy: 0.8207\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.3725 - sparse_categorical_accuracy: 0.8645\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3372 - sparse_categorical_accuracy: 0.8760\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3155 - sparse_categorical_accuracy: 0.8842\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2987 - sparse_categorical_accuracy: 0.8905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:upLogger:Model: \"sequential_2\"\n",
      "INFO:upLogger:_________________________________________________________________\n",
      "INFO:upLogger: Layer (type)                Output Shape              Param #   \n",
      "INFO:upLogger:=================================================================\n",
      "INFO:upLogger: flatten_2 (Flatten)         (None, 784)               0         \n",
      "INFO:upLogger:                                                                 \n",
      "INFO:upLogger: dense_4 (Dense)             (None, 64)                50240     \n",
      "INFO:upLogger:                                                                 \n",
      "INFO:upLogger: dense_5 (Dense)             (None, 64)                4160      \n",
      "INFO:upLogger:                                                                 \n",
      "INFO:upLogger: dense_6 (Dense)             (None, 10)                650       \n",
      "INFO:upLogger:                                                                 \n",
      "INFO:upLogger:=================================================================\n",
      "INFO:upLogger:Total params: 55050 (215.04 KB)\n",
      "INFO:upLogger:Trainable params: 55050 (215.04 KB)\n",
      "INFO:upLogger:Non-trainable params: 0 (0.00 Byte)\n",
      "INFO:upLogger:_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2nd Experiment : add another layer between the two dense layers.\n",
    "# Import and configure logging\n",
    "import logging\n",
    "import google.cloud.logging as cloud_logging\n",
    "from google.cloud.logging.handlers import CloudLoggingHandler\n",
    "from google.cloud.logging_v2.handlers import setup_logging\n",
    "up_logger = logging.getLogger('upLogger')\n",
    "up_logger.setLevel(logging.INFO)\n",
    "up_logger.addHandler(CloudLoggingHandler(cloud_logging.Client(), name=\"updated\"))\n",
    "\n",
    "# Import tensorflow_datasets\n",
    "import tensorflow_datasets as tfds\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define, load and configure data\n",
    "(ds_train, ds_test), info = tfds.load('fashion_mnist', split=['train', 'test'], with_info=True, as_supervised=True)\n",
    "# Define batch size\n",
    "BATCH_SIZE = 32\n",
    "# Normalizing and batch processing of data\n",
    "ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)\n",
    "ds_test = ds_test.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)\n",
    "# Define the model\n",
    "# add another layers : No significant impact -- because this is relatively simple data. For far more complex data, extra layers are often necessary\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "# Compile data\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(ds_train, epochs=5)\n",
    "# Logs model summary\n",
    "model.summary(print_fn=up_logger.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1a604fe-8a62-4aa4-9afd-ecd7215a90e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T07:50:13.339584Z",
     "iopub.status.busy": "2026-02-13T07:50:13.337768Z",
     "iopub.status.idle": "2026-02-13T07:50:55.922747Z",
     "shell.execute_reply": "2026-02-13T07:50:55.917460Z",
     "shell.execute_reply.started": "2026-02-13T07:50:13.339490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 2.7291 - sparse_categorical_accuracy: 0.6366\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.7585 - sparse_categorical_accuracy: 0.7093\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.6473 - sparse_categorical_accuracy: 0.7480\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5986 - sparse_categorical_accuracy: 0.7659\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5827 - sparse_categorical_accuracy: 0.7719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:upLogger:Model: \"sequential_3\"\n",
      "INFO:upLogger:_________________________________________________________________\n",
      "INFO:upLogger: Layer (type)                Output Shape              Param #   \n",
      "INFO:upLogger:=================================================================\n",
      "INFO:upLogger: flatten_3 (Flatten)         (None, 784)               0         \n",
      "INFO:upLogger:                                                                 \n",
      "INFO:upLogger: dense_7 (Dense)             (None, 64)                50240     \n",
      "INFO:upLogger:                                                                 \n",
      "INFO:upLogger: dense_8 (Dense)             (None, 10)                650       \n",
      "INFO:upLogger:                                                                 \n",
      "INFO:upLogger:=================================================================\n",
      "INFO:upLogger:Total params: 50890 (198.79 KB)\n",
      "INFO:upLogger:Trainable params: 50890 (198.79 KB)\n",
      "INFO:upLogger:Non-trainable params: 0 (0.00 Byte)\n",
      "INFO:upLogger:_________________________________________________________________\n",
      "2026-02-13 07:50:55.874831: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:upLogger:training images max 255\n",
      "INFO:upLogger:test images max 255\n"
     ]
    }
   ],
   "source": [
    "# 3rd Experiment : impact of removing normalization\n",
    "# Import and configure logging\n",
    "import logging\n",
    "import google.cloud.logging as cloud_logging\n",
    "from google.cloud.logging.handlers import CloudLoggingHandler\n",
    "from google.cloud.logging_v2.handlers import setup_logging\n",
    "up_logger = logging.getLogger('upLogger')\n",
    "up_logger.setLevel(logging.INFO)\n",
    "up_logger.addHandler(CloudLoggingHandler(cloud_logging.Client(), name=\"updated\"))\n",
    "\n",
    "# Import tensorflow_datasets\n",
    "import tensorflow_datasets as tfds\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define, load and configure data\n",
    "(ds_train, ds_test), info = tfds.load('fashion_mnist', split=['train', 'test'], with_info=True, as_supervised=True)\n",
    "# Define batch size\n",
    "BATCH_SIZE = 32\n",
    "# Normalizing and batch processing of data\n",
    "# remove the map function applied to both the training and test datasets\n",
    "# ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)\n",
    "# ds_test = ds_test.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)\n",
    "ds_train = ds_train.batch(BATCH_SIZE)\n",
    "ds_test = ds_test.batch(BATCH_SIZE)\n",
    "# Define the model\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "                                    # tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "# Compile data\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(ds_train, epochs=5)\n",
    "# Logs model summary\n",
    "model.summary(print_fn=up_logger.info)\n",
    "\n",
    "# Print out max value to see the changes\n",
    "image_batch, labels_batch = next(iter(ds_train))\n",
    "t_image_batch, t_labels_batch = next(iter(ds_test))\n",
    "up_logger.info(\"training images max \" + str(np.max(image_batch[0])))\n",
    "up_logger.info(\"test images max \" + str(np.max(t_image_batch[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97806ffc-418c-4a81-a0fb-f8ed84b8ff36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T07:53:40.695374Z",
     "iopub.status.busy": "2026-02-13T07:53:40.694532Z",
     "iopub.status.idle": "2026-02-13T07:53:43.834748Z",
     "shell.execute_reply": "2026-02-13T07:53:43.832243Z",
     "shell.execute_reply.started": "2026-02-13T07:53:40.695329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/var/tmp/ipykernel_64372/1255401726.py\", line 38, in <module>\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/losses.py\", line 2454, in sparse_categorical_crossentropy\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/backend.py\", line 5775, in sparse_categorical_crossentropy\n\nlogits and labels must have the same first dimension, got logits shape [25088,10] and labels shape [32]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_155030]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Compile data\u001b[39;00m\n\u001b[1;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(),\n\u001b[1;32m     36\u001b[0m               loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(),\n\u001b[1;32m     37\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mSparseCategoricalAccuracy()])\n\u001b[0;32m---> 38\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Logs model summary\u001b[39;00m\n\u001b[1;32m     40\u001b[0m model\u001b[38;5;241m.\u001b[39msummary(print_fn\u001b[38;5;241m=\u001b[39mup_logger\u001b[38;5;241m.\u001b[39minfo)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/var/tmp/ipykernel_64372/1255401726.py\", line 38, in <module>\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/losses.py\", line 2454, in sparse_categorical_crossentropy\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/backend.py\", line 5775, in sparse_categorical_crossentropy\n\nlogits and labels must have the same first dimension, got logits shape [25088,10] and labels shape [32]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_155030]"
     ]
    }
   ],
   "source": [
    "# 4th Experiment : impact of removing the Flatten() layer\n",
    "# Import and configure logging\n",
    "import logging\n",
    "import google.cloud.logging as cloud_logging\n",
    "from google.cloud.logging.handlers import CloudLoggingHandler\n",
    "from google.cloud.logging_v2.handlers import setup_logging\n",
    "up_logger = logging.getLogger('upLogger')\n",
    "up_logger.setLevel(logging.INFO)\n",
    "up_logger.addHandler(CloudLoggingHandler(cloud_logging.Client(), name=\"updated\"))\n",
    "\n",
    "# Import tensorflow_datasets\n",
    "import tensorflow_datasets as tfds\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define, load and configure data\n",
    "(ds_train, ds_test), info = tfds.load('fashion_mnist', split=['train', 'test'], with_info=True, as_supervised=True)\n",
    "# Define batch size\n",
    "BATCH_SIZE = 32\n",
    "# Normalizing and batch processing of data\n",
    "# remove the map function applied to both the training and test datasets\n",
    "# ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)\n",
    "# ds_test = ds_test.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)\n",
    "ds_train = ds_train.batch(BATCH_SIZE)\n",
    "ds_test = ds_test.batch(BATCH_SIZE)\n",
    "# Define the model\n",
    "model = tf.keras.models.Sequential([\n",
    "                                    # tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "                                    # tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "# Compile data\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(ds_train, epochs=5)\n",
    "# Logs model summary\n",
    "model.summary(print_fn=up_logger.info)\n",
    "\n",
    "# Print out max value to see the changes\n",
    "image_batch, labels_batch = next(iter(ds_train))\n",
    "t_image_batch, t_labels_batch = next(iter(ds_test))\n",
    "up_logger.info(\"training images max \" + str(np.max(image_batch[0])))\n",
    "up_logger.info(\"test images max \" + str(np.max(t_image_batch[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35123ba9-eb6d-4001-a394-a098cd8e8bd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T08:00:43.915120Z",
     "iopub.status.busy": "2026-02-13T08:00:43.914404Z",
     "iopub.status.idle": "2026-02-13T08:00:46.014346Z",
     "shell.execute_reply": "2026-02-13T08:00:46.001344Z",
     "shell.execute_reply.started": "2026-02-13T08:00:43.915066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-13 08:00:45.649181: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at sparse_xent_op.cc:103 : INVALID_ARGUMENT: Received a label value of 9 which is outside the valid range of [0, 5).  Label values: 2 1 8 4 1 9 2 2 0 2 6 9 0 7 5 4 0 1 8 0 4 2 6 7 0 6 4 0 3 1 2 7\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/var/tmp/ipykernel_64372/1321037406.py\", line 42, in <module>\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/losses.py\", line 2454, in sparse_categorical_crossentropy\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/backend.py\", line 5775, in sparse_categorical_crossentropy\n\nReceived a label value of 9 which is outside the valid range of [0, 5).  Label values: 2 1 8 4 1 9 2 2 0 2 6 9 0 7 5 4 0 1 8 0 4 2 6 7 0 6 4 0 3 1 2 7\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_156752]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Compile data\u001b[39;00m\n\u001b[1;32m     39\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(),\n\u001b[1;32m     40\u001b[0m               loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(),\n\u001b[1;32m     41\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mSparseCategoricalAccuracy()])\n\u001b[0;32m---> 42\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Logs model summary\u001b[39;00m\n\u001b[1;32m     44\u001b[0m model\u001b[38;5;241m.\u001b[39msummary(print_fn\u001b[38;5;241m=\u001b[39mup_logger\u001b[38;5;241m.\u001b[39minfo)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/var/tmp/ipykernel_64372/1321037406.py\", line 42, in <module>\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/losses.py\", line 2454, in sparse_categorical_crossentropy\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/backend.py\", line 5775, in sparse_categorical_crossentropy\n\nReceived a label value of 9 which is outside the valid range of [0, 5).  Label values: 2 1 8 4 1 9 2 2 0 2 6 9 0 7 5 4 0 1 8 0 4 2 6 7 0 6 4 0 3 1 2 7\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_156752]"
     ]
    }
   ],
   "source": [
    "# 5th Experiment : impact of different number of neurons in final (output) layer\n",
    "# Import and configure logging\n",
    "import logging\n",
    "import google.cloud.logging as cloud_logging\n",
    "from google.cloud.logging.handlers import CloudLoggingHandler\n",
    "from google.cloud.logging_v2.handlers import setup_logging\n",
    "up_logger = logging.getLogger('upLogger')\n",
    "up_logger.setLevel(logging.INFO)\n",
    "up_logger.addHandler(CloudLoggingHandler(cloud_logging.Client(), name=\"updated\"))\n",
    "\n",
    "# Import tensorflow_datasets\n",
    "import tensorflow_datasets as tfds\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define, load and configure data\n",
    "(ds_train, ds_test), info = tfds.load('fashion_mnist', split=['train', 'test'], with_info=True, as_supervised=True)\n",
    "# Define batch size\n",
    "BATCH_SIZE = 32\n",
    "# Normalizing and batch processing of data\n",
    "# remove the map function applied to both the training and test datasets\n",
    "# ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)\n",
    "# ds_test = ds_test.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y)).batch(BATCH_SIZE)\n",
    "ds_train = ds_train.batch(BATCH_SIZE)\n",
    "ds_test = ds_test.batch(BATCH_SIZE)\n",
    "# Define the model\n",
    "# the number of neurons in the last layer should match the number of classes you are classifying for. \n",
    "# In this case, it's the digits 0-9, so there are 10 of them, and hence you should have 10 neurons in your final layer.\n",
    "model = tf.keras.models.Sequential([\n",
    "                                    tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "                                    # tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "                                    # tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "                                    tf.keras.layers.Dense(5, activation=tf.nn.softmax)])\n",
    "\n",
    "# Compile data\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(ds_train, epochs=5)\n",
    "# Logs model summary\n",
    "model.summary(print_fn=up_logger.info)\n",
    "\n",
    "# Print out max value to see the changes\n",
    "image_batch, labels_batch = next(iter(ds_train))\n",
    "t_image_batch, t_labels_batch = next(iter(ds_test))\n",
    "up_logger.info(\"training images max \" + str(np.max(image_batch[0])))\n",
    "up_logger.info(\"test images max \" + str(np.max(t_image_batch[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5da8b8f-5307-43bf-8184-b755fa75c42b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m139",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m139"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
